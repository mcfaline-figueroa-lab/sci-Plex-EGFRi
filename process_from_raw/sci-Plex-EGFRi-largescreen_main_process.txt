# To follow this pipeline, edit environment/directory specifics, copy and paste the commands onto command line
# begin a session with 50gb memory
# srun --pty --time=08:00:00 --mem=50gb

# Define working directory where outputs and logs will be stored
WORKING_DIR=

# Define hash barcodes file and add to working directory (different for each experiment)
HASH_BARCODES_FILE=$WORKING_DIR/hashSampleSheet.txt

# Refer to scripts + references directory 
SCRIPTS_DIR=sci-RNA-seq/scripts
REF_DIR=sci-RNA-seq/reference


# STAR_INDEX is the path to the STAR genome index you want to align reads with.
# Create star indices for for the human genome 

# Used for the mapping of transcripts
# STAR_INDEX=$REF_DIR/human/

# GENE_MODEL_DIR is a directory with BED files for various genomic features
# that are used to assign aligned reads to genes.

# Used as the gene model 
GENE_MODEL_DIR=$REF_DIR/gene_model/human

# Ligation barcode file
LIG_BARCODES_FILE=$REF_DIR/ligation_384_3lvl

# Download GNU Datamash
# https://www.gnu.org/software/datamash/
DATAMASH_PATH=

# Change input path to bin where commands are found
# as in the original sci-plex demultiplexing pipeline:
# need to download packages for cutadapt, trimgalore, samtools, bedtools
BIN=<path_to_bin>

# Change this value to the number of batches to run at one time
BATCH_SIZE=10

# Give experiment desired name
SAMPLE_NAME="sciPlex"
echo "$SAMPLE_NAME" > $WORKING_DIR/combinatorial.indexing.key

### Creates directory to put slurm batch job logs
mkdir $WORKING_DIR/batch-logs
mkdir $WORKING_DIR/batch-logs/1-R1-info-R2-logs
mkdir $WORKING_DIR/batch-logs/2-trim-logs
mkdir $WORKING_DIR/batch-logs/3-STAR-logs
mkdir $WORKING_DIR/batch-logs/4-sam-sort-logs
mkdir $WORKING_DIR/batch-logs/5-count-rRNA-logs
mkdir $WORKING_DIR/batch-logs/6-make-bed-logs
mkdir $WORKING_DIR/batch-logs/7-assign-reads-logs
mkdir $WORKING_DIR/batch-logs/8-UMI-per-sample-logs
mkdir $WORKING_DIR/batch-logs/10-UMI-rollup-logs

#-------------------------------------------------------------------------------
# Put read 1 info (RT well, UMI) into read 2 read name
#-------------------------------------------------------------------------------
# Run steps separately between shortdT and randomN RT primers

cd $WORKING_DIR

RT_BARCODES_FILE=$REF_DIR/sci-RNA-seq3_RT_shortdT_plate2.txt

mkdir 2-combined-fastq-shortdT
mkdir 2-combined-fastq-shortdT/file-lists-for-r1-info-munging
 
# input folder containing fastqs to make batches of files to run
ls <path to fastqs> | grep -v Unassigned | split -l $BATCH_SIZE -d - 2-combined-fastq-shortdT/file-lists-for-r1-info-munging/
   
ls 2-combined-fastq-shortdT/file-lists-for-r1-info-munging | while read BATCH; do
   sbatch $SCRIPTS_DIR/2-put-read1-info-in-read2_3Level_element.sh \
       $WORKING_DIR/1-output-fastq/                                     \ # PATH_TO_FASTQS 
       $WORKING_DIR/2-combined-fastq-shortdT/file-lists-for-r1-info-munging/$BATCH      \
       $SCRIPTS_DIR/                                           \
       $RT_BARCODES_FILE                                       \
       $LIG_BARCODES_FILE                                      \
       $WORKING_DIR/combinatorial.indexing.key                 \
       $WORKING_DIR/2-combined-fastq-shortdT                            
 done

# -------------------------------------------------------------------------------
cd $WORKING_DIR
RT_BARCODES_FILE=$REF_DIR/sci-RNA-seq3_RT_randomN_plate2.txt
mkdir 2-combined-fastq-randomN
mkdir 2-combined-fastq-randomN/file-lists-for-r1-info-munging
 
ls 1-output-fastq/Samples/DefaultProject | grep -v Unassigned | grep -v Adept | grep -v DefaultProject | split -l $BATCH_SIZE -d - 2-combined-fastq-randomN/file-lists-for-r1-info-munging/
   
ls 2-combined-fastq-randomN/file-lists-for-r1-info-munging | while read BATCH; do
   sbatch $SCRIPTS_DIR/2-put-read1-info-in-read2_3Level_element.sh \
       $WORKING_DIR/1-output-fastq/                                     \ #PATH_TO_FASTQS 
       $WORKING_DIR/2-combined-fastq-randomN/file-lists-for-r1-info-munging/$BATCH      \
       $SCRIPTS_DIR/                                           \
       $RT_BARCODES_FILE                                       \
       $LIG_BARCODES_FILE                                      \
       $WORKING_DIR/combinatorial.indexing.key                 \
       $WORKING_DIR/2-combined-fastq-randomN                            
 done


#-------------------------------------------------------------------------------
# Trim poly-A tails 
#-------------------------------------------------------------------------------

cd $WORKING_DIR

mkdir $WORKING_DIR/3-trimmed-fastq-shortdT
mkdir $WORKING_DIR/3-trimmed-fastq-shortdT/file-lists-for-trimming

ls 1-output-fastq/Samples/DefaultProject | grep -v Unassigned | grep -v Adept | grep -v DefaultProject | sed 's/$/.fastq.gz/' | split -l $BATCH_SIZE -d - 3-trimmed-fastq-shortdT/file-lists-for-trimming/

ls 3-trimmed-fastq-shortdT/file-lists-for-trimming | while read BATCH; do
    sbatch $SCRIPTS_DIR/3-trim-STAR.sh     \
        $WORKING_DIR/2-combined-fastq-shortdT                        \
        $WORKING_DIR/3-trimmed-fastq-shortdT/file-lists-for-trimming/$BATCH         \
        $WORKING_DIR/3-trimmed-fastq-shortdT \
        $BIN
done

# -------------------------------------------------------------------------------

cd $WORKING_DIR

mkdir $WORKING_DIR/3-trimmed-fastq-randomN
mkdir $WORKING_DIR/3-trimmed-fastq-randomN/file-lists-for-trimming

ls 1-output-fastq/Samples/DefaultProject | grep -v Unassigned | grep -v Adept | grep -v DefaultProject | sed 's/$/.fastq.gz/' | split -l $BATCH_SIZE -d - 3-trimmed-fastq-randomN/file-lists-for-trimming/

ls 3-trimmed-fastq-randomN/file-lists-for-trimming | while read BATCH; do
    sbatch $SCRIPTS_DIR/3-trim-STAR.sh     \
        $WORKING_DIR/2-combined-fastq-randomN                       \
        $WORKING_DIR/3-trimmed-fastq-randomN/file-lists-for-trimming/$BATCH         \
        $WORKING_DIR/3-trimmed-fastq-randomN \
        $BIN
done

#-------------------------------------------------------------------------------
# Run STAR alignment
#-------------------------------------------------------------------------------

cd $WORKING_DIR

mkdir 4-aligned-reads-shortdT

sbatch $SCRIPTS_DIR/4-STAR-alignReads.sh \
    $WORKING_DIR/3-trimmed-fastq-shortdT        \
    $STAR_INDEX                          \
    $WORKING_DIR/4-aligned-reads-shortdT

# -------------------------------------------------------------------------------

cd $WORKING_DIR

mkdir 4-aligned-reads-randomN

sbatch $SCRIPTS_DIR/4-STAR-alignReads.sh \
    $WORKING_DIR/3-trimmed-fastq-randomN         \
    $STAR_INDEX                          \
    $WORKING_DIR/4-aligned-reads-randomN

#-------------------------------------------------------------------------------
# Filter ambiguously-mapped reads and sort BAM files
# Also count rRNA reads
#-------------------------------------------------------------------------------

cd $WORKING_DIR

mkdir 5-aligned-reads-filtered-sorted-shortdT
mkdir 5-aligned-reads-filtered-sorted-shortdT/file-lists-for-samtools-sort

ls 1-output-fastq/Samples/DefaultProject | grep -v Unassigned | grep -v Adept | grep -v DefaultProject | sed 's/$/.Aligned.out.bam/' |  split -l $BATCH_SIZE -d - 5-aligned-reads-filtered-sorted-shortdT/file-lists-for-samtools-sort/

ls 5-aligned-reads-filtered-sorted-shortdT/file-lists-for-samtools-sort | while read BATCH; do
    sbatch $SCRIPTS_DIR/5-samtools-filter-sort.sh    \
        $WORKING_DIR/4-aligned-reads-shortdT                              \
        $WORKING_DIR/5-aligned-reads-filtered-sorted-shortdT/file-lists-for-samtools-sort/$BATCH   \
        $WORKING_DIR/5-aligned-reads-filtered-sorted-shortdT       \
        $BIN
done

#
# You can run these jobs in parallel to the samtools-filter-sort.sh jobs
#

mkdir 6-rRNA-read-counts-shortdT

ls 5-aligned-reads-filtered-sorted-shortdT/file-lists-for-samtools-sort/ | while read BATCH; do
    sbatch $SCRIPTS_DIR/6-count-rRNA-reads.sh        \
        $WORKING_DIR/4-aligned-reads-shortdT                              \
        $WORKING_DIR/5-aligned-reads-filtered-sorted-shortdT/file-lists-for-samtools-sort/$BATCH        \
        $GENE_MODEL_DIR/latest.rRNA.gene.regions.union.bed      \
        $WORKING_DIR/6-rRNA-read-counts-shortdT         \
        $BIN
done

# -------------------------------------------------------------------------------

cd $WORKING_DIR

mkdir 5-aligned-reads-filtered-sorted-randomN
mkdir 5-aligned-reads-filtered-sorted-randomN/file-lists-for-samtools-sort

ls 1-output-fastq/Samples/DefaultProject | grep -v Unassigned | grep -v Adept | grep -v DefaultProject | sed 's/$/.Aligned.out.bam/' |  split -l $BATCH_SIZE -d - 5-aligned-reads-filtered-sorted-randomN/file-lists-for-samtools-sort/

ls 5-aligned-reads-filtered-sorted-randomN/file-lists-for-samtools-sort | while read BATCH; do
    sbatch $SCRIPTS_DIR/5-samtools-filter-sort.sh    \
        $WORKING_DIR/4-aligned-reads-randomN                              \
        $WORKING_DIR/5-aligned-reads-filtered-sorted-randomN/file-lists-for-samtools-sort/$BATCH   \
        $WORKING_DIR/5-aligned-reads-filtered-sorted-randomN       \
        $BIN
done

#
# You can run these jobs in parallel to the samtools-filter-sort.sh jobs
#

mkdir 6-rRNA-read-counts-randomN

ls 5-aligned-reads-filtered-sorted-randomN/file-lists-for-samtools-sort/ | while read BATCH; do
    sbatch $SCRIPTS_DIR/6-count-rRNA-reads.sh        \
        $WORKING_DIR/4-aligned-reads-randomN                              \
        $WORKING_DIR/5-aligned-reads-filtered-sorted-randomN/file-lists-for-samtools-sort/$BATCH        \
        $GENE_MODEL_DIR/latest.rRNA.gene.regions.union.bed      \
        $WORKING_DIR/6-rRNA-read-counts-randomN         \
        $BIN
done

#-------------------------------------------------------------------------------
# Split reads in BAM files into BED intervals
#-------------------------------------------------------------------------------

cd $WORKING_DIR

mkdir 7-aligned-reads-rmdup-split-bed-shortdT
mkdir 7-aligned-reads-rmdup-split-bed-shortdT/file-lists-for-rmdup

ls 1-output-fastq/Samples/DefaultProject | grep -v Unassigned | grep -v Adept | grep -v DefaultProject | sed 's/$/.bam/' | split -l $BATCH_SIZE -d - 7-aligned-reads-rmdup-split-bed-shortdT/file-lists-for-rmdup/

ls 7-aligned-reads-rmdup-split-bed-shortdT/file-lists-for-rmdup | while read BATCH; do
    sbatch $SCRIPTS_DIR/7-rmdup-and-make-split-bed.sh    \
        $WORKING_DIR/5-aligned-reads-filtered-sorted-shortdT                  \
        $WORKING_DIR/7-aligned-reads-rmdup-split-bed-shortdT/file-lists-for-rmdup/$BATCH                    \
        $SCRIPTS_DIR/                                               \
        $WORKING_DIR/7-aligned-reads-rmdup-split-bed-shortdT      \
        $BIN
done

# -------------------------------------------------------------------------------

cd $WORKING_DIR

mkdir 7-aligned-reads-rmdup-split-bed-randomN
mkdir 7-aligned-reads-rmdup-split-bed-randomN/file-lists-for-rmdup

ls 1-output-fastq/Samples/DefaultProject | grep -v Unassigned | grep -v Adept | grep -v DefaultProject | sed 's/$/.bam/' | split -l $BATCH_SIZE -d - 7-aligned-reads-rmdup-split-bed-randomN/file-lists-for-rmdup/

ls 7-aligned-reads-rmdup-split-bed-randomN/file-lists-for-rmdup | while read BATCH; do
    sbatch $SCRIPTS_DIR/7-rmdup-and-make-split-bed.sh    \
        $WORKING_DIR/5-aligned-reads-filtered-sorted-randomN                  \
        $WORKING_DIR/7-aligned-reads-rmdup-split-bed-randomN/file-lists-for-rmdup/$BATCH                    \
        $SCRIPTS_DIR/                                               \
        $WORKING_DIR/7-aligned-reads-rmdup-split-bed-randomN      \
        $BIN
done

#-------------------------------------------------------------------------------
# Assign reads to genes, using the BED files as input
#-------------------------------------------------------------------------------

mkdir $WORKING_DIR/7-aligned-reads-rmdup-split-bed-gencode-shortdT
FILE_LIST=$(ls $WORKING_DIR/7-aligned-reads-rmdup-split-bed-shortdT/ | grep -v file-lists)
FILE=($FILE_LIST)

# in way that index was built, "chr" needs to be added before chromosome number (i.e. turn 1 into chr1, etc.)
for i in "${FILE[@]}"
 do
  awk 'OFS="\t" {$1="chr"$1; print}' $WORKING_DIR/7-aligned-reads-rmdup-split-bed-shortdT/$i \
       > $WORKING_DIR/7-aligned-reads-rmdup-split-bed-gencode-shortdT/$i

  echo "Processed $i" 
 done

cd $WORKING_DIR

mkdir 8-unique-read-to-gene-assignments-shortdT
mkdir 8-unique-read-to-gene-assignments-shortdT/file-lists-for-assign-reads-to-genes

ls 7-aligned-reads-rmdup-split-bed-shortdT/ | grep "[.]bed$" | split -l $BATCH_SIZE -d - 8-unique-read-to-gene-assignments-shortdT/file-lists-for-assign-reads-to-genes/

ls 8-unique-read-to-gene-assignments-shortdT/file-lists-for-assign-reads-to-genes | while read BATCH; do
    sbatch $SCRIPTS_DIR/8-assign-reads-to-genes.sh       \
        $WORKING_DIR/7-aligned-reads-rmdup-split-bed-gencode-shortdT                  \
        $WORKING_DIR/8-unique-read-to-gene-assignments-shortdT/file-lists-for-assign-reads-to-genes/$BATCH    \
        $GENE_MODEL_DIR/latest.exons.bed                            \
        $GENE_MODEL_DIR/latest.genes.bed                            \
        $SCRIPTS_DIR/                                               \
        $WORKING_DIR/8-unique-read-to-gene-assignments-shortdT              \
        $BIN                                                        \
        $DATAMASH_PATH
done

# -------------------------------------------------------------------------------

mkdir $WORKING_DIR/7-aligned-reads-rmdup-split-bed-gencode-randomN
FILE_LIST=$(ls $WORKING_DIR/7-aligned-reads-rmdup-split-bed-randomN/ | grep -v file-lists)
FILE=($FILE_LIST)

for i in "${FILE[@]}"
 do
  awk 'OFS="\t" {$1="chr"$1; print}' $WORKING_DIR/7-aligned-reads-rmdup-split-bed-randomN/$i \
       > $WORKING_DIR/7-aligned-reads-rmdup-split-bed-gencode-randomN/$i

  echo "Processed $i" 
 done

cd $WORKING_DIR

mkdir 8-unique-read-to-gene-assignments-randomN
mkdir 8-unique-read-to-gene-assignments-randomN/file-lists-for-assign-reads-to-genes

ls 7-aligned-reads-rmdup-split-bed-randomN/ | grep "[.]bed$" | split -l $BATCH_SIZE -d - 8-unique-read-to-gene-assignments-randomN/file-lists-for-assign-reads-to-genes/

ls 8-unique-read-to-gene-assignments-randomN/file-lists-for-assign-reads-to-genes | while read BATCH; do
    sbatch $SCRIPTS_DIR/8-assign-reads-to-genes.sh       \
        $WORKING_DIR/7-aligned-reads-rmdup-split-bed-gencode-randomN                  \
        $WORKING_DIR/8-unique-read-to-gene-assignments-randomN/file-lists-for-assign-reads-to-genes/$BATCH    \
        $GENE_MODEL_DIR/latest.exons.bed                            \
        $GENE_MODEL_DIR/latest.genes.bed                            \
        $SCRIPTS_DIR/                                               \
        $WORKING_DIR/8-unique-read-to-gene-assignments-randomN              \
        $BIN                                                        \
        $DATAMASH_PATH
done

#-------------------------------------------------------------------------------
# Compute the duplication rate and proportion of reads that are from rRNA
#-------------------------------------------------------------------------------

cd $WORKING_DIR

mkdir 9-UMI-counts-by-sample-shortdT/
mkdir 9-UMI-counts-by-sample-shortdT/file-lists-for-UMI-counting

ls 7-aligned-reads-rmdup-split-bed-gencode-shortdT/ | while read FILE; do
    PCR_WELL=`basename $FILE .bed`
    echo "$PCR_WELL"
done \
| split -l $BATCH_SIZE -d - 9-UMI-counts-by-sample-shortdT/file-lists-for-UMI-counting/

ls 9-UMI-counts-by-sample-shortdT/file-lists-for-UMI-counting | while read BATCH; do
    sbatch $SCRIPTS_DIR/9-count-UMI-per-sample.sh    \
        $WORKING_DIR/5-aligned-reads-filtered-sorted-shortdT              \
        $WORKING_DIR/7-aligned-reads-rmdup-split-bed-shortdT              \
        $WORKING_DIR/9-UMI-counts-by-sample-shortdT/file-lists-for-UMI-counting/$BATCH         \
        $WORKING_DIR/9-UMI-counts-by-sample-shortdT                       \
        $BIN                                                      \
        $DATAMASH_PATH
done

#-------------------------------------------------------------------------------
cd $WORKING_DIR

mkdir 9-UMI-counts-by-sample-randomN/
mkdir 9-UMI-counts-by-sample-randomN/file-lists-for-UMI-counting

ls 7-aligned-reads-rmdup-split-bed-gencode-randomN/ | while read FILE; do
    PCR_WELL=`basename $FILE .bed`
    echo "$PCR_WELL"
done \
| split -l $BATCH_SIZE -d - 9-UMI-counts-by-sample-randomN/file-lists-for-UMI-counting/

ls 9-UMI-counts-by-sample-randomN/file-lists-for-UMI-counting | while read BATCH; do
    sbatch $SCRIPTS_DIR/9-count-UMI-per-sample.sh    \
        $WORKING_DIR/5-aligned-reads-filtered-sorted-randomN              \
        $WORKING_DIR/7-aligned-reads-rmdup-split-bed-randomN              \
        $WORKING_DIR/9-UMI-counts-by-sample-randomN/file-lists-for-UMI-counting/$BATCH         \
        $WORKING_DIR/9-UMI-counts-by-sample-randomN                       \
        $BIN                                                      \
        $DATAMASH_PATH
done

#-------------------------------------------------------------------------------
# Getting final rRNA and UMI counts
#-------------------------------------------------------------------------------
cd $WORKING_DIR
mkdir 11-final-output-shortdT

cat 9-UMI-counts-by-sample-shortdT/*.UMI.count | sort -k1,1 \
| $DATAMASH_PATH -g 1 sum 2 \
>11-final-output-shortdT/total.UMI.count.by.sample

cat 9-UMI-counts-by-sample-shortdT/*.read.count | sort -k1,1 \
| $DATAMASH_PATH -g 1 sum 2 \
>11-final-output-shortdT/total.read.count.by.sample


cat 6-rRNA-read-counts-shortdT/* | sort -k1,1 | $DATAMASH_PATH -g 1 sum 2 sum 3 \
| join - 11-final-output-shortdT/total.UMI.count.by.sample \
| join - 11-final-output-shortdT/total.read.count.by.sample \
| awk 'BEGIN {
    printf "%-18s    %11s    %8s    %10s    %8s\n",
        "sample", "n.reads", "pct.rRNA", "n.UMI", "dup.rate";
} {
    printf "%-18s    %11d    %7.1f%%    %10d    %7.1f%%\n",
        $1, $3, 100 * $2/$3, $4, 100 * (1 - $4/$5);
}' \
>11-final-output-shortdT/rRNA.and.dup.rate.stats

cat 11-final-output-shortdT/rRNA.and.dup.rate.stats

#-------------------------------------------------------------------------------
cd $WORKING_DIR
mkdir 11-final-output-randomN

cat 9-UMI-counts-by-sample-randomN/*.UMI.count | sort -k1,1 \
| $DATAMASH_PATH -g 1 sum 2 \
>11-final-output-randomN/total.UMI.count.by.sample

cat 9-UMI-counts-by-sample-randomN/*.read.count | sort -k1,1 \
| $DATAMASH_PATH -g 1 sum 2 \
>11-final-output-randomN/total.read.count.by.sample


cat 6-rRNA-read-counts-randomN/* | sort -k1,1 | $DATAMASH_PATH -g 1 sum 2 sum 3 \
| join - 11-final-output-randomN/total.UMI.count.by.sample \
| join - 11-final-output-randomN/total.read.count.by.sample \
| awk 'BEGIN {
    printf "%-18s    %11s    %8s    %10s    %8s\n",
        "sample", "n.reads", "pct.rRNA", "n.UMI", "dup.rate";
} {
    printf "%-18s    %11d    %7.1f%%    %10d    %7.1f%%\n",
        $1, $3, 100 * $2/$3, $4, 100 * (1 - $4/$5);
}' \
>11-final-output-randomN/rRNA.and.dup.rate.stats

cat 11-final-output-randomN/rRNA.and.dup.rate.stats

cat 11-final-output-shortdT/rRNA.and.dup.rate.stats 11-final-output-randomN/rRNA.and.dup.rate.stats \
| grep -v sample \
| $DATAMASH_PATH -W -g 1 sum 2 sum 4 \
| awk 'BEGIN {printf "%10s  %-11s  %-10s\n", "sample", "n.reads", "n.UMI";} 
             {printf "%10s  %-12d  %-11d\n", "Combined", $2, $3;}'

#-------------------------------------------------------------------------------
# Make the final UMI count matrix
#-------------------------------------------------------------------------------
cd $WORKING_DIR

mkdir 10-UMI-count-rollup
mkdir 10-UMI-count-rollup/file-lists-for-UMI-count-rollup

mkdir 8-unique-read-to-gene-assignments-all

cd $WORKING_DIR/8-unique-read-to-gene-assignments-shortdT/
for file in *; do
    cat $file > ../8-unique-read-to-gene-assignments-all/$file
done

cd $WORKING_DIR/8-unique-read-to-gene-assignments-randomN/
for file in *; do
    cat $file >> ../8-unique-read-to-gene-assignments-all/$file
done

cd $WORKING_DIR
ls 8-unique-read-to-gene-assignments-all/ | grep -v file-lists | split -l $BATCH_SIZE -d - 10-UMI-count-rollup/file-lists-for-UMI-count-rollup/
ls 10-UMI-count-rollup/file-lists-for-UMI-count-rollup | while read BATCH; do
    sbatch $SCRIPTS_DIR/10-UMI-count-rollup.sh        \
        $WORKING_DIR/8-unique-read-to-gene-assignments-all            \
        $WORKING_DIR/10-UMI-count-rollup/file-lists-for-UMI-count-rollup/$BATCH     \
        $WORKING_DIR/10-UMI-count-rollup              \
        $DATAMASH_PATH
done
 
#-------------------------------------------------------------------------------
cd $WORKING_DIR

mkdir 11-final-output
cp $GENE_MODEL_DIR/latest.gene.annotations 11-final-output/gene.annotations

rm -rf 10-UMI-count-rollup/file-lists-for-UMI-count-rollup/
cat 10-UMI-count-rollup/* | gzip > prelim.UMI.count.rollup.gz

#
# Make samples.to.exclude file.
# Each line lists a sample name to exclude.
# You can leave it empty.
#

touch samples.to.exclude

#
# This uses the UMI_PER_CELL_CUTOFF variable
# defined in the knee plot section.
#

UMI_PER_CELL_CUTOFF=0
echo "UMI_PER_CELL_CUTOFF = $UMI_PER_CELL_CUTOFF"

gunzip < prelim.UMI.count.rollup.gz \
| $DATAMASH_PATH -g 1 sum 3 \
| tr '|' '\t' \
| awk -v CUTOFF=$UMI_PER_CELL_CUTOFF '
    ARGIND == 1 {
        exclude[$1] = 1;
    } $3 >= CUTOFF && !($1 in exclude) {
        print $2 "\t" $1; 
    }' samples.to.exclude - \
| sort -k1,1 -S 4G \
>11-final-output/cell.annotations

gunzip < prelim.UMI.count.rollup.gz \
| tr '|' '\t' \
| awk '{
    if (ARGIND == 1) {
        gene_idx[$1] = FNR;
    } else if (ARGIND == 2) {
        cell_idx[$1] = FNR;
    } else if ($2 in cell_idx) {
        printf "%d\t%d\t%d\n",
            gene_idx[$3], cell_idx[$2], $4;
    }
}' 11-final-output/gene.annotations 11-final-output/cell.annotations - \
>11-final-output/UMI.count.matrix

# To create a cds object without cell line annotation and without hash for now
# R packages need to be installed -- tidyverse and monocle3

module load R/4.1.0
module load gdal
module load geos

Rscript $SCRIPTS_DIR/11-makeCDS.R \
        $WORKING_DIR/11-final-output/UMI.count.matrix \
        $WORKING_DIR/11-final-output/gene.annotations \
        $WORKING_DIR/11-final-output/cell.annotations \
        $WORKING_DIR/11-final-output

#-------------------------------------------------------------------------------
# Parse hash barcodes
#-------------------------------------------------------------------------------
# Begin processing hash

SAMPLE_NAME="sciPlex" # should match SAMPLE_NAME in transcriptome reads
BATCH_SIZE=10
mkdir $WORKING_DIR/batch-logs/hash

cd $WORKING_DIR

mkdir hash
mkdir hash/hashed-fastq

ls 3-trimmed-fastq-shortdT/file-lists-for-trimming | while read BATCH; do
    sbatch $SCRIPTS_DIR/parse_hash.sh \
        $WORKING_DIR/2-combined-fastq-shortdT-4run                             \
        $WORKING_DIR/3-trimmed-fastq-shortdT/file-lists-for-trimming/$BATCH             \
        $SCRIPTS_DIR/                                           \
        $HASH_BARCODES_FILE                                     \
        $WORKING_DIR/combinatorial.indexing.key                 \
        $WORKING_DIR/hash/hashed-fastq
done


#-------------------------------------------------------------------------------
# Finish parse hash barcodes
#-------------------------------------------------------------------------------
cd $WORKING_DIR
mkdir $WORKING_DIR/hash/hash-final

zcat hash/hashed-fastq/*.gz \
    | $DATAMASH_PATH -g 2,3,4 count 3 \
    | $DATAMASH_PATH -g 1 sum 4 \
    | awk -v S=$SAMPLE_NAME '{OFS="\t";} {print S, $0}' \
    > $WORKING_DIR/hash/hash-final/hashReads.per.cell


zcat hash/hashed-fastq/*.gz \
    | uniq \
    | $DATAMASH_PATH -g 2,3,4 count 3 \
    | $DATAMASH_PATH -g 1 sum 4 \
    | awk -v S=$SAMPLE_NAME '{OFS="\t";} {print S, $0}' \
    > $WORKING_DIR/hash/hash-final/hashUMIs.per.cell


zcat hash/hashed-fastq/*.gz \
    | uniq \
    | $DATAMASH_PATH -g 1,2,4 count 3  \
    > $WORKING_DIR/hash/hash-final/hashTable.out 

paste $WORKING_DIR/hash/hash-final/hashUMIs.per.cell  $WORKING_DIR/hash/hash-final/hashReads.per.cell \
     | cut -f 1,2,6,3 \
     | awk 'BEGIN {OFS="\t";} {dup = 1-($3/$4); print $1,$2,$3,$4,dup;}' \
     > $WORKING_DIR/hash/hash-final/hashDupRate.txt

# Some operations to look into hash stats
cat hash/hash-final/hashDupRate.txt | $DATAMASH_PATH sum 3 # tells you the UMI sum
cat hash/hash-final/hashDupRate.txt | $DATAMASH_PATH sum 4 # tells you the read sum
cat hash/hash-final/hashDupRate.txt | $DATAMASH_PATH median 5 # tells you the median duplication rate

# Post-hash counting total reads
cat hash/hash-final/hashDupRate.txt \
| $DATAMASH_PATH sum 3 sum 4 \
| awk 'BEGIN {printf "%10s  %-11s  %-10s\n", "sample", "n.reads", "n.UMI";} 
             {printf "%10s  %-12d  %-11d\n", "Hash", $2, $1;}' > hash/hash-final/hashStats.txt

cat 11-final-output-shortdT/rRNA.and.dup.rate.stats 11-final-output-randomN/rRNA.and.dup.rate.stats \
| grep -v sample \
| $DATAMASH_PATH -W -g 1 sum 2 sum 4 \
| awk 'BEGIN {} {printf "%10s  %-12d  %-11d\n", "dT+randN", $2, $3;}' \
| cat hash/hash-final/hashStats.txt - \
| awk '{for(i=2;i<=NF;i++)a[i]+=$i;print $0} END{l="SUM";i=2;while(i in a){l="   "l"  "a[i];i++};print l}' \
> 11-final-output/final.run.stats


#-------------------------------------------------------------------------------
# END OF PIPELINE
# preprocessed cds and hashTable.out can be joined and subjected to QC locally
